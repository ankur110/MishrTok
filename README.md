# MishrTok

![Language](https://img.shields.io/badge/Language-Python-blue)
![Efficiency](https://img.shields.io/badge/Efficiency-1.14x_vs_GPT4o-brightgreen)
![Vocab](https://img.shields.io/badge/Vocab_Size-32k-orange)

**MishrTok** (à¤®à¤¿à¤¶à¥à¤° = Mixed) â€“ A custom Byte Pair Encoding (BPE) tokenizer optimized for **code-mixed Romanized Hinglish** and **Devanagari Hindi**.

Trained on a large Hinglish corpus with intelligent regex-based pre-tokenization, this 32k tokenizer achieves **~14.1% better token efficiency** than OpenAI's state-of-the-art `o200k_base` (GPT-4o) tokenizer on diverse Hinglish and Hindi text â€” all with a vocab size 6Ã— smaller.

This project is heavily inspired by **Andrej Karpathy's** ["Let's build the GPT tokenizer"](https://www.youtube.com/watch?v=kCc8FmEb1nY) and his clean [`minbpe`](https://github.com/karpathy/minbpe) implementation.

## Why This Tokenizer?

General-purpose tokenizers struggle with Romanized Hinglish and Devanagari Hindi because:
- Common Hindi words/phrases ("bhai", "yaar", "tension na le", "à¤œà¤¼à¤¿à¤‚à¤¦à¤—à¥€", etc.) get overly fragmented.
- Code-mixing and script-mixing patterns are underrepresented in English-dominated training data.
- Older tokenizers (cl100k, p50k) completely fall apart on pure Devanagari text.

This tokenizer uses:
- A powerful regex to pre-tokenize URLs, mentions, hashtags, emojis, numbers, **Devanagari script blocks**, and separate Latin/Romanized words.
- Frequency-weighted BPE merges on real Hinglish data.
- Result: Fewer tokens â†’ faster inference, lower costs, better context utilization for Hindi/Hinglish LLMs.

## Features

- Regex pre-tokenization (handles mixed scripts, emojis, social media artifacts)
- Supports both Romanized Hinglish and pure Devanagari Hindi efficiently
- Frequency-filtered pre-tokens (`min_word_freq`, `max_unique_words`)
- Training checkpoints
- Clean encode/decode with perfect round-trip
- **Outperforms** `o200k_base` (GPT-4o), `cl100k_base`, and older OpenAI tokenizers on Hinglish/Hindi

## ğŸ“Š Benchmark Results

We compared `MishrTok` against OpenAI's latest `o200k_base` (GPT-4o) on a diverse test set.

| Task Category | MishrTok Tokens | GPT-4o Tokens | Efficiency |
| :--- | :---: | :---: | :--- |
| **Casual Chat** | 93 | 103 | âœ… **1.11x** |
| **Tech Discussion** | 90 | 91 | âœ… **1.01x** |
| **Emotional Rant** | 107 | 129 | âœ… **1.21x** |
| **Hardcore Hinglish** | 70 | 95 | âœ… **1.36x** |
| **Pure Hindi** | 60 | 70 | âœ… **1.17x** |
| **TOTAL** | **510** | **582** | âœ… **1.14x** |

> **Verdict:** MishrTok is **14.1% more efficient** than GPT-4o on mixed-script text.


## ğŸ“‚ Project Structure

```text
Hinglish-Tokenizer/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ds.ipynb                  # Dataset preparation / exploration notebook
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hinglish_32k.model        # Trained tokenizer merges & metadata
â”‚   â””â”€â”€ hinglish_32k.vocab.json   # Vocab (hex-encoded bytes)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ HinglishBPE.py            # Core tokenizer class (Train/Encode/Decode)
â”‚   â””â”€â”€ training.py               # Training script
â”œâ”€â”€ inference.py                  # Comprehensive benchmark vs OpenAI
â””â”€â”€ README.md

```

## ğŸ› ï¸ Quick Start

### 1. Installation

This project does not require a heavy environment. Just install the dependencies:

```bash
pip install regex tqdm tiktoken datasets

```

### 2. Usage

You can load the pre-trained model and start encoding immediately.

```python
from src.HinglishBPE import HinglishBPE

# Load the tokenizer (ensure path is correct)
tok = HinglishBPE()
tok.load("models/hinglish_32k")

text = "à¤µà¤¿à¤œà¥à¤à¤¾à¤¨ à¤”à¤° à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€ à¤•à¥‡ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤®à¥‡à¤‚ à¤¹à¤®à¤¨à¥‡ à¤¬à¤¹à¥à¤¤ à¤ªà¥à¤°à¤—à¤¤à¤¿ à¤•à¥€ à¤¹à¥ˆà¥¤"

# Encode
ids = tok.encode(text)
print(f"Tokens ({len(ids)}): {ids}")

# Decode (Round-trip check)
decoded = tok.decode(ids)
print(f"Decoded: {decoded}")
assert text == decoded

```

### 3. Running Benchmarks

To reproduce the efficiency results on your own machine:

```bash
python inference.py

```

### 4. Training from Scratch

If you have a custom corpus (e.g., `corpus.txt`), you can retrain the model:

```python
from src.HinglishBPE import HinglishBPE

tok = HinglishBPE()
tok.train(
    filename="data/corpus.txt",
    vocab_size=32768,
    min_word_freq=2,
    max_unique_words=3_000_000,
    verbose=True,
    checkpoint_prefix="hinglish_32k_chk",
)
tok.save("models/hinglish_32k")

```

## ğŸ§  Acknowledgments

* **Andrej Karpathy:** For the clean [`minbpe`](https://github.com/karpathy/minbpe) architecture and educational resources.
* **OpenAI:** For `tiktoken`, used here for benchmarking comparisons.
* **The Indic NLP Community:** For creating datasets like HinGE and IndicCorp that made this training possible.


